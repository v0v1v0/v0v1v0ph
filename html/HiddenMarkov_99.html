<div class="container">

<table style="width: 100%;"><tr>
<td>Viterbi</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Viterbi Algorithm for Hidden Markov Model</h2>

<h3>Description</h3>

<p>Provides methods for the generic function <code>Viterbi</code>. This predicts the most likely sequence of Markov states given the observed dataset. There is currently no method for objects of class <code>"mmpp"</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'dthmm'
Viterbi(object, ...)
## S3 method for class 'mmglm0'
Viterbi(object, ...)
## S3 method for class 'mmglm1'
Viterbi(object, ...)
## S3 method for class 'mmglmlong1'
Viterbi(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>an object with class <code>"dthmm"</code>, <code>"mmglm0"</code>, <code>"mmglm1"</code> or <code>"mmglmlong1"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>other arguments.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The purpose of the Viterbi algorithm is to <em>globally decode</em> the underlying hidden Markov state at each time point. It does this by determining the sequence of states <code class="reqn">(k_1^*, \cdots, k_n^*)</code> which maximises the joint distribution of the hidden states given the entire observed process, i.e.
</p>
<p style="text-align: center;"><code class="reqn">
(k_1^*, \cdots, k_n^*) = _{\stackrel{\mbox{argmax}}{{k_1, \cdots, k_n \in \{1, 2, \cdots, m\}}}} \Pr\{ C_1=k_1, \cdots, C_n=k_n \,|\, X^{(n)}=x^{(n)} \}\,.
</code>
</p>

<p>The algorithm has been taken from Zucchini (2005), however, we calculate sums of the logarithms of probabilities rather than products of probabilities. This lessens the chance of numerical underflow. Given that the logarithmic function is monotonically increasing, the <em>argmax</em> will still be the same. Note that <em>argmax</em> can be evaluated with the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function <code>which.max</code>.
</p>
<p>Determining the <em>a posteriori</em> most probable state at time <code class="reqn">i</code> is referred to as <em>local decoding</em>, i.e.
</p>
<p style="text-align: center;"><code class="reqn">
k_i^* = _{\stackrel{\mbox{argmax}}{k \in \{1, 2, \cdots, m\}}} \Pr\{ C_i=k \,|\, X^{(n)}=x^{(n)} \}\,.
</code>
</p>

<p>Note that the above probabilities are calculated by the function <code>Estep</code>, and are contained in <code>u[i,j]</code> (output from <code>Estep</code>), i.e. <code class="reqn">k_i^*</code> is simply <code>which.max(u[i,])</code>.
</p>
<p>The code for the methods <code>"dthmm"</code>, <code>"mmglm0"</code>, <code>"mmglm1"</code> and <code>"mmglmlong1"</code> can be viewed by appending <code>Viterbi.dthmm</code>, <code>Viterbi.mmglm0</code>, <code>Viterbi.mmglm1</code> or <code>Viterbi.mmglmlong1</code>, respectively, to <code>HiddenMarkov:::</code>, on the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> command line; e.g. <code>HiddenMarkov:::dthmm</code>. The three colons are needed because these method functions are not in the exported NAMESPACE.
</p>


<h3>Value</h3>

<p>A vector of length <code class="reqn">n</code> containing integers (<code class="reqn">1, \cdots, m</code>) representing the hidden Markov states for each node of the chain.
</p>


<h3>References</h3>

<p>Cited references are listed on the HiddenMarkov manual page.
</p>


<h3>Examples</h3>

<pre><code class="language-R">Pi &lt;- matrix(c(1/2, 1/2,   0,   0,   0,
               1/3, 1/3, 1/3,   0,   0,
                 0, 1/3, 1/3, 1/3,   0,
                 0,   0, 1/3, 1/3, 1/3,
                 0,   0,   0, 1/2, 1/2),
             byrow=TRUE, nrow=5)
delta &lt;- c(0, 1, 0, 0, 0)
lambda &lt;- c(1, 4, 2, 5, 3)
m &lt;- nrow(Pi)

x &lt;- dthmm(NULL, Pi, delta, "pois", list(lambda=lambda), discrete=TRUE)
x &lt;- simulate(x, nsim=2000)

#------  Global Decoding  ------

states &lt;- Viterbi(x)
states &lt;- factor(states, levels=1:m)

#  Compare predicted states with true states
#  p[j,k] = Pr{Viterbi predicts state k | true state is j}
p &lt;- matrix(NA, nrow=m, ncol=m)
for (j in 1:m){
    a &lt;- (x$y==j)
    p[j,] &lt;- table(states[a])/sum(a)
}
print(p)

#------  Local Decoding  ------

#   locally decode at i=100

print(which.max(Estep(x$x, Pi, delta, "pois", list(lambda=lambda))$u[100,]))

#---------------------------------------------------
#   simulate a beta HMM

Pi &lt;- matrix(c(0.8, 0.2,
               0.3, 0.7),
             byrow=TRUE, nrow=2)
delta &lt;- c(0, 1)

y &lt;- seq(0.01, 0.99, 0.01)
plot(y, dbeta(y, 2, 6), type="l", ylab="Density", col="blue")
points(y, dbeta(y, 6, 2), type="l", col="red")

n &lt;- 100
x &lt;- dthmm(NULL, Pi, delta, "beta",
           list(shape1=c(2, 6), shape2=c(6, 2)))
x &lt;- simulate(x, nsim=n)

#   colour denotes actual hidden Markov state
plot(1:n, x$x, type="l", xlab="Time", ylab="Observed Process")
points((1:n)[x$y==1], x$x[x$y==1], col="blue", pch=15)
points((1:n)[x$y==2], x$x[x$y==2], col="red", pch=15)

states &lt;- Viterbi(x)
#   mark the wrongly predicted states
wrong &lt;- (states != x$y)
points((1:n)[wrong], x$x[wrong], pch=1, cex=2.5, lwd=2)
</code></pre>


</div>