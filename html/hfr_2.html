<div class="container">

<table style="width: 100%;"><tr>
<td>hfr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit a hierarchical feature regression</h2>

<h3>Description</h3>

<p>HFR is a regularized regression estimator that decomposes a least squares
regression along a supervised hierarchical graph, and shrinks the edges of the
estimated graph to regularize parameters. The algorithm leads to group shrinkage in the
regression parameters and a reduction in the effective model degrees of freedom.
</p>


<h3>Usage</h3>

<pre><code class="language-R">hfr(
  x,
  y,
  weights = NULL,
  kappa = 1,
  q = NULL,
  intercept = TRUE,
  standardize = TRUE,
  partial_method = c("pairwise", "shrinkage"),
  l2_penalty = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Input matrix or data.frame, of dimension <code class="reqn">(N\times p)</code>; each row is an observation vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector. If non-NULL, weighted least squares is used for the level-specific regressions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kappa</code></td>
<td>
<p>The target effective degrees of freedom of the regression as a percentage of <code class="reqn">p</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>Thinning parameter representing the quantile cut-off (in terms of contributed variance) above which to consider levels in the hierarchy. This can used to reduce the number of levels in high-dimensional problems. Default is no thinning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Should intercept be fitted. Default is <code>intercept=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Logical flag for x variable standardization prior to fitting the model. The coefficients are always returned on the original scale. Default is <code>standardize=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partial_method</code></td>
<td>
<p>Indicate whether to use pairwise partial correlations, or shrinkage partial correlations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l2_penalty</code></td>
<td>
<p>Optional penalty for level-specific regressions (useful in high-dimensional case)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>hclust</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Shrinkage can be imposed by targeting an explicit effective degrees of freedom.
Setting the argument <code>kappa</code> to a value between <code>0</code> and <code>1</code> controls
the effective degrees of freedom of the fitted object as a percentage of <code class="reqn">p</code>.
When <code>kappa</code> is <code>1</code> the result is equivalent to the result from an ordinary
least squares regression (no shrinkage). Conversely, <code>kappa</code> set to <code>0</code>
represents maximum shrinkage.
</p>
<p>When <code class="reqn">p &gt; N</code> <code>kappa</code> is a percentage of <code class="reqn">(N - 2)</code>.
</p>
<p>If no <code>kappa</code> is set, a linear regression with <code>kappa = 1</code> is
estimated.
</p>
<p>Hierarchical clustering is performed using <code>hclust</code>. The default is set to
ward.D2 clustering but can be overridden by passing a method argument to <code>...</code>.
</p>
<p>For high-dimensional problems, the hierarchy becomes very large. Setting <code>q</code> to a value below 1
reduces the number of levels used in the hierarchy. <code>q</code> represents a quantile-cutoff of the amount of
variation contributed by the levels. The default (<code>q = NULL</code>) considers all levels.
</p>
<p>When data exhibits multicollinearity it can be useful to include a penalty on the l2 norm in the level-specific regressions.
This can be achieved by setting the <code>l2_penalty</code> parameter.
</p>


<h3>Value</h3>

<p>An 'hfr' regression object.
</p>


<h3>Author(s)</h3>

<p>Johann Pfitzinger
</p>


<h3>References</h3>

<p>Pfitzinger, Johann (2024). Cluster Regularization via a Hierarchical Feature Regression. _Econometrics and Statistics_ (in press). URL https://doi.org/10.1016/j.ecosta.2024.01.003.
</p>


<h3>See Also</h3>

<p><code>cv.hfr</code>, <code>se.avg</code>, <code>coef</code>, <code>plot</code> and <code>predict</code> methods
</p>


<h3>Examples</h3>

<pre><code class="language-R">x = matrix(rnorm(100 * 20), 100, 20)
y = rnorm(100)
fit = hfr(x, y, kappa = 0.5)
coef(fit)

</code></pre>


</div>