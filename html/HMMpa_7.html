<div class="container">

<table style="width: 100%;"><tr>
<td>direct_numerical_maximization</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Estimation by Directly Maximizing the log-Likelihood 
</h2>

<h3>Description</h3>

<p>Estimates the parameters of a (stationary) discrete-time hidden Markov model by directly maximizing the log-likelihood of the model using the nlm-function. See MacDonald &amp; Zucchini (2009, Paragraph 3) for further details.
</p>


<h3>Usage</h3>

<pre><code class="language-R">direct_numerical_maximization(x, m, delta, gamma, 
     distribution_class, distribution_theta, 
     DNM_limit_accuracy = 0.001, DNM_max_iter = 50, 
     DNM_print = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>a vector object containing the time-series of observations that are assumed to be realizations of the (hidden Markov state dependent) observation process of the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>a (finite) number of states in the hidden Markov chain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>

<p>a vector object containing starting values for the marginal probability distribution of the <code>m</code> states of the Markov chain at the time point <code>t=1</code>. This implementation of the algorithm uses the stationary distribution as delta.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>

<p>a matrix (<code>nrow=ncol=m</code>) containing starting values for the transition matrix of the hidden Markov chain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the <code>m</code> observation distributions of the Markov dependent observation process. The following distributions are supported by this algorithm: Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>); normal (<code>norm</code>, discrete log-Likelihood not applicable by this algorithm).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution_theta</code></td>
<td>

<p>a list object containing starting values for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DNM_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the direct numerical maximization algorithm using the nlm-function. Default value is <code>0.001</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DNM_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations of the direct numerical maximization using the nlm-function. Default value is <code>50</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DNM_print</code></td>
<td>

<p>a single numerical value to determine the level of printing of the <code>nlm</code>-function.  See <code>nlm</code>-function for further informations. The value <code>0</code> suppresses, that no printing will be outputted. Default value is <code>2</code> for full printing.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>direct_numerical_maximization </code> returns a list containing the estimated parameters of the hidden Markov model and other components.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>input time-series of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>input number of hidden states in the Markov chain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logL</code></td>
<td>

<p>a numerical value representing the logarithmized likelihood calculated by the <code>forward_backward_algorithm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AIC</code></td>
<td>

<p>a numerical value representing Akaike's information criterion for the hidden Markov model with estimated parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>BIC</code></td>
<td>

<p>a numerical value representing the Bayesian information criterion for the hidden Markov model with estimated parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>

<p>a vector object containing the estimates for the marginal probability distribution of the <code>m</code> states of the Markov chain at time-point point <code>t=1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>

<p>a matrix containing the estimates for the transition matrix of the hidden Markov chain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution_theta</code></td>
<td>

<p>a list object containing estimates for the parameters of the <code>m</code> observation distributions that are dependent on the hidden Markov state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution_class</code></td>
<td>
<p>input distribution class.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>The basic algorithm of a Poisson-HMM is provided by MacDonald &amp; Zucchini (2009, Paragraph A.1). Extension and implementation by Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L., Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code>HMM_based_method</code>, 
<code>HMM_training</code>, 
<code>Baum_Welch_algorithm</code>, 
<code>forward_backward_algorithm</code>,
</p>
<p><code>initial_parameter_training</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


### Assummptions (number of states, probability vector, 
### transition matrix, and distribution parameters)
    m &lt;-4
delta &lt;- c(0.25,0.25,0.25,0.25)
gamma &lt;- 0.7 * diag(m) + rep(0.3 / m)
distribution_class &lt;- "pois"
distribution_theta &lt;- list(lambda = c(4,9,17,25))

### Estimation of a HMM using the method of 
### direct numerical maximization

trained_HMM_with_m_hidden_states &lt;- 
		direct_numerical_maximization(x = x, 
      m = m, 
      delta = delta, 
      gamma = gamma, 
      distribution_class = distribution_class,
      DNM_max_iter=100,
      distribution_theta = distribution_theta)

print(trained_HMM_with_m_hidden_states)

</code></pre>


</div>