<div class="container">

<table style="width: 100%;"><tr>
<td>HPLB</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>High Probability Lower Bounds (HPLB) for the Total Variation Distance (TV) Based on Finite Samples</h2>

<h3>Description</h3>

<p>Implementations of different HPLBs for TV as described in (Michel et al., 2020).
</p>


<h3>Usage</h3>

<pre><code class="language-R">HPLB(
  t,
  rho,
  s = 0.5,
  estimator.type = "adapt",
  alpha = 0.05,
  tv.seq = seq(from = 0, to = 1, by = 1/length(t)),
  custom.bounding.seq = NULL,
  direction = rep("left", length(s)),
  cutoff = 0.5,
  verbose.plot = FALSE,
  seed = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>t</code></td>
<td>
<p>a numeric vector value corresponding to a natural ordering of the observations. For a two-sample
test 0-1 numeric values values should be provided.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>a numeric vector value providing an ordering. This could be
a binary classifier, a regressor, a witness function from a MMD kernel or anything else that would witness a distributional difference.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>a numeric vector value giving split points on t.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimator.type</code></td>
<td>
<p>a character value indicating which estimator to use. One option out of:
</p>

<ul>
<li>
<p><code>adapt</code>:adaptive binary classification estimator (asymptotic bounding function)
</p>
</li>
<li>
<p><code>bayes</code>:binary classification estimator
</p>
</li>
<li>
<p><code>bayes_finite_sample</code>:binary classification finite sample estimator
</p>
</li>
<li>
<p><code>adapt_empirical</code>:adaptive binary classification estimator (simulation-based bounding function)
</p>
</li>
<li>
<p><code>adapt_custom</code>:adaptive binary classificatrion estimator (user-defined bounding function)
</p>
</li>
<li>
<p><code>adapt_dwit</code>:adaptive binary classificatrion estimator (for distributional witnesses estimation)
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>a numeric value giving the overall type-I error control level.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tv.seq</code></td>
<td>
<p>a sequence of values between 0 and 1 used as the grid search for the total variation distance in case of tv-search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>custom.bounding.seq</code></td>
<td>
<p>a list of bounding functions respecting the order of tv.seq used in case of estimator.type "custom-tv-search".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>direction</code></td>
<td>
<p>a character vector value made of "left" or "right" giving which distribution witness count to estimate (t&lt;=s or t&gt;s?).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoff</code></td>
<td>
<p>a numeric value. This is the cutoff used if bayes estimators are used. The theory suggests to use 1/2 but this can be changed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose.plot</code></td>
<td>
<p>a boolean value for additional plots.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>an integer value. The seed for reproducibility.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters for the function <code>empiricalBF</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a <code>list</code> containing the relevant lower bounds estimates. For the total variation distance the relevant entry is <code>tvhat</code>.
</p>


<h3>Author(s)</h3>

<p>Loris Michel, Jeffrey Naef
</p>


<h3>References</h3>

<p>L. Michel, J. Naef and N. Meinshausen (2020). High-Probability Lower Bounds for the Total Variation Distance <br></p>


<h3>Examples</h3>

<pre><code class="language-R">## libs
library(HPLB)
library(ranger)
library(distrEx)

## reproducibility
set.seed(0)

## Example 1: TV lower bound based on two samples (bayes estimator), Gaussian mean-shift example

n &lt;- 100
means &lt;- rep(c(0,2), each = n / 2)
x &lt;- stats::rnorm(n, mean = means)
t &lt;- rep(c(0,1), each = n / 2)

bayesRate &lt;- function(x) {
  return(stats::dnorm(x, mean = 2) /
    (stats::dnorm(x, mean = 2) + stats::dnorm(x, mean = 0)))
}

# estimated HPLB
tvhat &lt;- HPLB(t = t, rho = bayesRate(x), estimator.type = "bayes")
# true TV
TotalVarDist(e1 = Norm(2,1), e2 = Norm(0,1))

## Example 2: optimal mixture detection (adapt estimator), Gaussian mean-shift example

n &lt;- 100
mean.shift &lt;- 2
t.train &lt;- runif(n, 0 ,1)
x.train &lt;- ifelse(t.train&gt;0.5, stats::rnorm(n, mean.shift), stats::rnorm(n))
rf &lt;- ranger::ranger(t~x, data.frame(t=t.train,x=x.train))

n &lt;- 100
t.test &lt;- runif(n, 0 ,1)
x.test &lt;- ifelse(t.test&gt;0.5, stats::rnorm(n, mean.shift), stats::rnorm(n))
rho &lt;- predict(rf, data.frame(t=t.test,x=x.test))$predictions

## out-of-sample
tv.oos &lt;- HPLB(t = t.test, rho = rho, s = seq(0.1,0.9,0.1), estimator.type = "adapt")


## total variation values
tv &lt;- c()
for (s in seq(0.1,0.9,0.1)) {

 if (s&lt;=0.5) {

   D.left &lt;- Norm(0,1)
 } else {

   D.left &lt;- UnivarMixingDistribution(Dlist = list(Norm(0,1),Norm(mean.shift,1)),
               mixCoeff = c(ifelse(s&lt;=0.5, 1, 0.5/s), ifelse(s&lt;=0.5, 0, (s-0.5)/s)))
 }
 if (s &lt; 0.5) {

   D.right &lt;- UnivarMixingDistribution(Dlist = list(Norm(0,1),Norm(mean.shift,1)),
               mixCoeff = c(ifelse(s&lt;=0.5, (0.5-s)/(1-s), 0), ifelse(s&lt;=0.5, (0.5/(1-s)), 1)))
 } else {

   D.right &lt;- Norm(mean.shift,1)
 }
tv &lt;- c(tv, TotalVarDist(e1 = D.left, e2 = D.right))
}

## plot
oldpar &lt;- par(no.readonly =TRUE)
par(mfrow=c(2,1))
plot(t.test,x.test,pch=19,xlab="t",ylab="x")
plot(seq(0.1,0.9,0.1), tv.oos$tvhat,type="l",ylim=c(0,1),xlab="t", ylab="TV")
lines(seq(0.1,0.9,0.1), tv, col="red",type="l")
par(oldpar)

</code></pre>


</div>