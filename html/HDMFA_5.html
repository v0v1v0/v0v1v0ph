<div class="container">

<table style="width: 100%;"><tr>
<td>MHFA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Matrix Huber Factor Analysis
</h2>

<h3>Description</h3>

<p>This function is to fit the matrix factor models via the Huber loss. We propose two algorithms to do robust factor analysis. One is based on minimizing the Huber loss of the idiosyncratic error's Frobenius norm, which leads to a weighted iterative projection approach to compute and learn the parameters and thereby named as Robust-Matrix-Factor-Analysis (RMFA). The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm (IHR).
</p>


<h3>Usage</h3>

<pre><code class="language-R">MHFA(X, W1=NULL, W2=NULL, m1, m2, method, max_iter = 100, ep = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W1</code></td>
<td>

<p>Only if <code>method="E"</code>, the inital value of row loadings matrix. The default is NULL, which is randomly chosen and all entries from a standard normal distribution.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W2</code></td>
<td>

<p>Only if <code>method="E"</code>, the inital value of column loadings matrix. The default is NULL, which is randomly chosen and all entries from a standard normal distribution.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m1</code></td>
<td>

<p>A positive integer indicating the row factor numbers.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m2</code></td>
<td>

<p>A positive integer indicating the column factor numbers.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>Character string, specifying the type of the estimation method to be used.
</p>

<dl>
<dt><code>"P",</code></dt>
<dd>
<p>indicates minimizing the Huber loss of the idiosyncratic error's Frobenius norm. (RMFA)</p>
</dd>
<dt><code>"E",</code></dt>
<dd>
<p>indicates minimizing the elementwise Huber loss. (IHR)</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>

<p>Only if <code>method="E"</code>, the maximum number of iterations in the iterative Huber regression algorithm. The default is 100.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ep</code></td>
<td>

<p>Only if <code>method="E"</code>, the stopping critetion parameter in the iterative Huber regression algorithm. The default is <code class="reqn">10^{-4} \times Tp_1 p_2</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For the matrix factor models, He et al. (2021) propose a weighted iterative projection approach to compute and learn the parameters by minimizing the Huber loss function of the idiosyncratic error's Frobenius norm. In details, for observations <code class="reqn">\bold{X}_t, t=1,2,\cdots,T</code>, define 
</p>
<p style="text-align: center;"><code class="reqn">\bold{M}_c^w = \frac{1}{Tp_2} \sum_{t=1}^T w_t \bold{X}_t \bold{C} \bold{C}^\top \bold{X}_t^\top, \bold{M}_r^w = \frac{1}{Tp_1} \sum_{t=1}^T w_t \bold{X}_t^\top \bold{R} \bold{R}^\top \bold{X}_t.</code>
</p>
 
<p>The estimators of loading matrics <code class="reqn">\hat{\bold{R}}</code> and <code class="reqn">\hat{\bold{C}}</code> are calculated by <code class="reqn">\sqrt{p_1}</code> times the leading <code class="reqn">k_1</code> eigenvectors of <code class="reqn">\bold{M}_c^w</code> and <code class="reqn">\sqrt{p_2}</code> times the leading <code class="reqn">k_2</code> eigenvectors of <code class="reqn">\bold{M}_r^w</code>.
And </p>
<p style="text-align: center;"><code class="reqn">\hat{\bold{F}}_t=\frac{1}{p_1 p_2}\hat{\bold{R}}^\top \bold{X}_t \hat{\bold{C}}.</code>
</p>
<p> For details, see He et al. (2023). 
</p>
<p>The other one is based on minimizing the element-wise Huber loss. Define 
</p>
<p style="text-align: center;"><code class="reqn">M_{i,Tp_2}(\bold{r}, \bold{F}_t, \bold{C})=\frac{1}{Tp_2} \sum_{t=1}^{T} \sum_{j=1}^{p_2} H_\tau \left(x_{t,ij}-\bold{r}_i^\top\bold{F}_t\bold{c}_j \right),</code>
</p>

<p style="text-align: center;"><code class="reqn">M_{i,Tp_1}(\bold{R}, \bold{F}_t, \bold{c})=\frac{1}{Tp_1}\sum_{t=1}^T\sum_{i=1}^{p_1} H_\tau \left(x_{t,ij}-\bold{r}_i^\top\bold{F}_t\bold{c}_j\right),</code>
</p>

<p style="text-align: center;"><code class="reqn">M_{t,p_1 p_2}(\bold{R}, \mathrm{vec}(\bold{F}), \bold{C})=\frac{1}{p_1 p_2} \sum_{i=1}^{p_1}\sum_{j=1}^{p_2} H_\tau \left(x_{t,ij}-(\bold{c}_j \otimes \bold{r}_i)^\top \mathrm{vec}(\bold{F})\right).</code>
</p>
<p> This can be seen as Huber regression as each time optimizing one argument while keeping the other two fixed.
</p>


<h3>Value</h3>

<p>The return value is a list. In this list, it contains the following:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>F</code></td>
<td>
<p>The estimated factor matrix of dimension <code class="reqn">T \times m_1\times m_2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>
<p>The estimated row loading matrix of dimension <code class="reqn">p_1\times m_1</code>, satisfying <code class="reqn">\bold{R}^\top\bold{R}=p_1\bold{I}_{m_1}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C</code></td>
<td>
<p>The estimated column loading matrix of dimension <code class="reqn">p_2\times m_2</code>, satisfying <code class="reqn">\bold{C}^\top\bold{C}=p_2\bold{I}_{m_2}</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>He, Y., Kong, X., Yu, L., Zhang, X., &amp; Zhao, C. (2023). Matrix factor analysis: From least squares to iterative projection. Journal of Business &amp; Economic Statistics, 1-26.
</p>
<p>He, Y., Kong, X. B., Liu, D., &amp; Zhao, R. (2023). Robust Statistical Inference for Large-dimensional Matrix-valued Time Series via Iterative Huber Regression. &lt;arXiv:2306.03317&gt;.
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   #Estimate the factor matrices and loadings by RMFA
   fit1=MHFA(X, m1=3, m2=3, method="P")
   Rhat1=fit1$R 
   Chat1=fit1$C
   Fhat1=fit1$F
   
   #Estimate the factor matrices and loadings by IHR
   fit2=MHFA(X, W1=NULL, W2=NULL, 3, 3, "E")
   Rhat2=fit2$R 
   Chat2=fit2$C
   Fhat2=fit2$F
   
   #Estimate the common component by RMFA
   CC1=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC1[t,,]=Rhat1%*%Fhat1[t,,]%*%t(Chat1)
   }
   CC1
   
   #Estimate the common component by IHR
   CC2=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC2[t,,]=Rhat2%*%Fhat2[t,,]%*%t(Chat2)
   }
   CC2
</code></pre>


</div>