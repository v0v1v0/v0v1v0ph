<div class="container">

<table style="width: 100%;"><tr>
<td>HiDimDA-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>High Dimensional Discriminant Analysis</h2>

<h3>Description</h3>

<p>Performs Linear Discriminant Analysis in High Dimensional problems based on reliable covariance estimators for problems with
(many) more variables than observations. Includes routines for classifier training, prediction, cross-validation and variable selection.</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> HiDimDA</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 0.2-7</td>
</tr>
<tr>
<td style="text-align: left;">
Date: </td>
<td style="text-align: left;"> 2024-10-06</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
<td style="text-align: left;">
LazyLoad: </td>
<td style="text-align: left;"> yes</td>
</tr>
<tr>
<td style="text-align: left;">
LazyData: </td>
<td style="text-align: left;"> yes</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p>HiDimDA is a package for High-Dimensional Discriminant Analysis aimed at problems with many variables, possibly much more
than the number of available observations. Its core consists of the four Linear Discriminant Analyis routines:
</p>

<table>
<tr>
<td style="text-align: left;">
 Dlda: </td>
<td style="text-align: left;"> Diagonal Linear Discriminant Analysis</td>
</tr>
<tr>
<td style="text-align: left;">
Slda: </td>
<td style="text-align: left;"> Shrunken Linear Discriminant Analysis</td>
</tr>
<tr>
<td style="text-align: left;">
Mlda: </td>
<td style="text-align: left;"> Maximum-uncertainty Linear Discriminant Analysis</td>
</tr>
<tr>
<td style="text-align: left;">
RFlda: </td>
<td style="text-align: left;"> Factor-model  Linear Discriminant Analysis</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p>and the variable selection routine:
</p>

<table>
<tr>
<td style="text-align: left;">
SelectV: </td>
<td style="text-align: left;"> High-Dimensional variable selection for supervised classification</td>
</tr>
<tr>
<td style="text-align: left;"> 
</td>
</tr>
</table>
<p>that selects variables to be used in a Discriminant classification rule by
ranking them according to two-sample t-scores (problems with two-groups),
or ANOVA F-scores (problems wih more that two groups), and discarding those
with scores below a threshold defined by the Higher Criticism (HC) approach 
of Donoho and Jin (2008), the Expanded Higher Criticism scheme 
proposed by Duarte Silva (2011), False Discovery Rate (Fdr) control as suggested by 
Benjamini and Hochberg (1995), the FAIR approach of Fan and Fan (2008), or simply by 
fixing the number of retained variables to some pre-defined constant.
</p>
<p>All four discriminant routines, ‘Dlda’, ‘Slda’, ‘Mlda’ and ‘RFlda’, compute Linear
Discriminant Functions, by default after a preliminary variable selection step, based on alternative estimators of 
a within-groups covariance matrix that leads to reliable allocation rules in problems where the number of selected
variables is close to, or larger than, the number of available observations.  
</p>
<p>Consider a Discriminant Analysis problem with <code class="reqn">k</code> groups, <code class="reqn">p</code> selected variables, a training sample consisting
of <code class="reqn">N = \sum_{g=1}^{k}n_g</code> observations with group and overall means, 
<code class="reqn">\bar{X}_g</code> and <code class="reqn">\bar{X}_.</code>, and a between-groups scatter (scaled by degrees of freedom) 
matrix, <code class="reqn">S_B = \frac{1}{N-k} \sum_{g=1}^{k} n_g (\bar{X}_g -\bar{X}_.)(\bar{X}_g -\bar{X}_.)^T </code>
</p>
<p>Following the two main classical approaches to Linear Discrimant Analysis, the Discriminant Functions returned by HiDimDA discriminant
routines are either based on the canonical linear discriminants given by the normalized eigenvectors
</p>
<p style="text-align: center;"><code class="reqn">LD_j = Egvct_j (S_B \hat{\Sigma}_W^{-1})</code>
</p>
  
<p style="text-align: center;"><code class="reqn">j = 1,...,r=min(p,k-1)</code>
</p>

<p style="text-align: center;"><code class="reqn">[LD_1, ..., LD_r]^T \hat{\Sigma}_W [LD_1, ..., LD_r] = I_r </code>
</p>
 
<p>or the classification functions 
</p>
<p style="text-align: center;"><code class="reqn">CF_g = (\bar{X}_g - \bar{X}_1) \hat{\Sigma}_W^{-1}</code>
</p>
 
<p style="text-align: center;"><code class="reqn">g = 2,...,k</code>
</p>
 
<p>where <code class="reqn">\hat{\Sigma}_W^{-1}</code> is an estimate of  the inverse within-groups covariance. 
</p>
<p>It is well known that these two approaches are equivalent, in the sense that classification rules that assign new observations to
the group with the closest (according to the Euclidean distance) centroid in the space of the canonical variates, 
<code class="reqn">Z = [LD_1 ... LD_r]^T X </code>, give the same results as the rule that assigns a new observation to group 1 if all classification scores, 
<code class="reqn">Clscr_g = CF_g^T X - CF_g^T  \frac{(\bar{X}_1 + \bar{X}_g)}{2} </code>, are negative, and to the group with the highest classification 
score otherwise.
</p>
<p>The discriminant routines of HiDimDA compute canonical linear discriminant functions by default, and classification functions when
the argument ‘ldafun’ is set to “classification”. However, unlike traditional linear discriminant analysis where
<code class="reqn">\Sigma_W^{-1}</code> is estimated by the inverse of the sample covariance, 
which is not well-defined when <code class="reqn">p \geq N-k</code> and is unreliable if <code class="reqn">p</code> is close to <code class="reqn">N-k</code>, the routines of HiDimDA use 
four alternative well-conditioned estimators of <code class="reqn">\Sigma_W^{-1}</code> that lead to reliable classification rules if <code class="reqn">p</code> is larger than, 
or close to, <code class="reqn">N-k</code>.      
</p>
<p>In particular, ‘Dlda’ estimates <code class="reqn">\Sigma_W^{-1}</code> by the diagonal matrix of inverse sample variances, ‘Slda’ by
the inverse of an optimally shrunken Ledoit and Wolf's (2004) covariance estimate with the targets and optimal
target intensity estimators proposed by Fisher and Sun (2011), ‘Mlda’ uses a regularized inverse
covariance that deemphasizes the importance given to the last eigenvectors of the sample covariance (see Thomaz, Kitani
and Gillies (2006) for details), and ‘RFlda’ uses a factor model estimate of the true inverse correlation (or covariance)
matrix based on the approach of Duarte Silva (2011).
</p>
<p>The HiDimDA package also includes predict methods for all discriminant routines implemented, a routine (‘DACrossVal’) for asssessing
the quality of the classification results by kfold cross-validation, and utilities for storing, extracting and efficiently handling  specialized high-dimensional covariance and inverse covariance matrix estimates. 
</p>


<h3>Author(s)</h3>

<p>Antonio Pedro Duarte Silva  &lt;psilva@porto.ucp.pt&gt;
</p>
<p>Maintainer: Antonio Pedro Duarte Silva  &lt;psilva@porto.ucp.pt&gt;
</p>


<h3>References</h3>

<p>Benjamini, Y. and Hochberg, Y. (1995) “Controling the false discovery rate: A practical and powerful
approach to multiple testing”, <em>Journal of the Royal Statistical Society</em> B, 57, 289-300.
</p>
<p>Donoho, D. and Jin, J. (2008) “Higher criticism thresholding: Optimal feature selection when useful
features are rare and weak”, In: <em>Proceedings National Academy of Sciences</em>, USA 105, 14790-14795.
</p>
<p>Fan, J. and Fan, Y. (2008) “High-dimensional classification using features annealed independence rules”,
<em>Annals of Statistics</em>, 36 (6), 2605-2637.
</p>
<p>Fisher, T.J. and Sun, X. (2011) “Improved Stein-type shrinkage estimators for the high-dimensional multivariate normal covariance matrix”, <em>Computational Statistics and Data Analysis</em>, 55 (1), 1909-1918. 
</p>
<p>Ledoit, O. and Wolf, M. (2004) “A well-conditioned estimator for large-dimensional covariance matrices.”, <em>Journal of Multivariate Analysis</em>, 88 (2), 365-411. 
</p>
<p>Pedro Duarte Silva, A. (2011) “Two Group Classification with High-Dimensional Correlated Data: A Factor Model Approach”, 
<em>Computational Statistics and Data Analysis</em>, 55 (1), 2975-2990.
</p>
<p>Thomaz, C.E. Kitani, E.C. and Gillies, D.F. (2006) “A maximum uncertainty LDA-based approach for limited sample size problems - with application to face recognition”, <em>Journal of the Brazilian Computer Society</em>, 12 (2), 7-18
</p>


<h3>See Also</h3>

<p><code>Dlda</code>, <code>Mlda</code>, <code>Slda</code>,<code>RFlda</code>, <code>predict.canldaRes</code>, <code>predict.clldaRes</code>, <code>AlonDS</code></p>


<h3>Examples</h3>

<pre><code class="language-R">
# train the four main classifiers with their default setings 
# on Alon's colon data set (after a logarithmic transformation), 
# selecting genes by the Expanded HC scheme 

# Pre-process and select the genes to be used in the classifiers

log10genes &lt;- log10(AlonDS[,-1]) 
SelectionRes &lt;- SelectV(log10genes,AlonDS$grouping)
genesused &lt;- log10genes[SelectionRes$vkpt]

# Train classifiers

DiaglldaRule &lt;- Dlda(genesused,AlonDS$grouping)     
FactldaRule &lt;- RFlda(genesused,AlonDS$grouping)     
MaxUldaRule &lt;- Mlda(genesused,AlonDS$grouping)     
ShrkldaRule &lt;- Slda(genesused,AlonDS$grouping)     

# Get in-sample classification results

predict(DiaglldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       
predict(FactldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       
predict(MaxUldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       
predict(ShrkldaRule,genesused,grpcodes=levels(AlonDS$grouping))$class         	       

# Compare classifications with true assignments

cat("Original classes:\n")
print(AlonDS$grouping)             		 

# Show set of selected genes

cat("Genes kept in discrimination rule:\n")
print(colnames(genesused))             		 
cat("Number of selected genes =",SelectionRes$nvkpt,"\n")
</code></pre>


</div>