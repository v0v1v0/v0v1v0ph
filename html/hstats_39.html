<div class="container">

<table style="width: 100%;"><tr>
<td>perm_importance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Permutation Importance</h2>

<h3>Description</h3>

<p>Calculates permutation importance for a set of features or a set of feature groups.
By default, importance is calculated for all columns in <code>X</code> (except column names
used as response <code>y</code> or as case weight <code>w</code>).
</p>


<h3>Usage</h3>

<pre><code class="language-R">perm_importance(object, ...)

## Default S3 method:
perm_importance(
  object,
  X,
  y,
  v = NULL,
  pred_fun = stats::predict,
  loss = "squared_error",
  m_rep = 4L,
  agg_cols = FALSE,
  normalize = FALSE,
  n_max = 10000L,
  w = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'ranger'
perm_importance(
  object,
  X,
  y,
  v = NULL,
  pred_fun = function(m, X, ...) stats::predict(m, X, ...)$predictions,
  loss = "squared_error",
  m_rep = 4L,
  agg_cols = FALSE,
  normalize = FALSE,
  n_max = 10000L,
  w = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'explainer'
perm_importance(
  object,
  X = object[["data"]],
  y = object[["y"]],
  v = NULL,
  pred_fun = object[["predict_function"]],
  loss = "squared_error",
  m_rep = 4L,
  agg_cols = FALSE,
  normalize = FALSE,
  n_max = 10000L,
  w = object[["weights"]],
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Fitted model object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>pred_fun(object, X, ...)</code>,
for instance <code>type = "response"</code> in a <code>glm()</code> model, or <code>reshape = TRUE</code> in a
multiclass XGBoost model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>A data.frame or matrix serving as background dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector/matrix of the response, or the corresponding column names in <code>X</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>Vector of feature names, or named list of feature groups.
The default (<code>NULL</code>) will use all column names of <code>X</code> with the following exception:
If <code>y</code> or <code>w</code> are passed  as column names, they are dropped.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred_fun</code></td>
<td>
<p>Prediction function of the form <code style="white-space: pre;">⁠function(object, X, ...)⁠</code>,
providing <code class="reqn">K \ge 1</code> predictions per row. Its first argument represents the
model <code>object</code>, its second argument a data structure like <code>X</code>. Additional arguments
(such as <code>type = "response"</code> in a GLM, or <code>reshape = TRUE</code> in a multiclass XGBoost
model) can be passed via <code>...</code>. The default, <code>stats::predict()</code>, will work in
most cases.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>One of "squared_error", "logloss", "mlogloss", "poisson",
"gamma", or "absolute_error". Alternatively, a loss function
can be provided that turns observed and predicted values into a numeric vector or
matrix of unit losses of the same length as <code>X</code>.
For "mlogloss", the response <code>y</code> can either be a dummy matrix or a discrete vector.
The latter case is handled via a fast version of <code>model.matrix(~ as.factor(y) + 0)</code>.
For "squared_error", the response can be a factor with levels in column order of
the predictions. In this case, squared error is evaluated for each one-hot-encoded column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m_rep</code></td>
<td>
<p>Number of permutations (default 4).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>agg_cols</code></td>
<td>
<p>Should multivariate losses be summed up? Default is <code>FALSE</code>.
In combination with the squared error loss, <code>agg_cols = TRUE</code> gives
the Brier score for (probabilistic) classification.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>Should importance statistics be divided by average loss?
Default is <code>FALSE</code>. If <code>TRUE</code>, an importance of 1 means that the average loss
has been doubled by shuffling that feature's column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_max</code></td>
<td>
<p>If <code>X</code> has more than <code>n_max</code> rows, a random sample of <code>n_max</code> rows is
selected from <code>X</code>. In this case, set a random seed for reproducibility.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>Optional vector of case weights. Can also be a column name of <code>X</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Should a progress bar be shown? The default is <code>TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The permutation importance of a feature is defined as the increase in the average
loss when shuffling the corresponding feature values before calculating predictions.
By default, the process is repeated <code>m_rep = 4</code> times, and the results are averaged.
In most of the cases, importance values should be derived from an independent test
data set. Set <code>normalize = TRUE</code> to get <em>relative</em> increases in average loss.
</p>


<h3>Value</h3>

<p>An object of class "hstats_matrix" containing these elements:
</p>

<ul>
<li> <p><code>M</code>: Matrix of statistics (one column per prediction dimension), or <code>NULL</code>.
</p>
</li>
<li> <p><code>SE</code>: Matrix with standard errors of <code>M</code>, or <code>NULL</code>.
Multiply with <code>sqrt(m_rep)</code> to get <em>standard deviations</em> instead.
Currently, supported only for <code>perm_importance()</code>.
</p>
</li>
<li> <p><code>m_rep</code>: The number of repetitions behind standard errors <code>SE</code>, or <code>NULL</code>.
Currently, supported only for <code>perm_importance()</code>.
</p>
</li>
<li> <p><code>statistic</code>: Name of the function that generated the statistic.
</p>
</li>
<li> <p><code>description</code>: Description of the statistic.
</p>
</li>
</ul>
<h3>Methods (by class)</h3>


<ul>
<li> <p><code>perm_importance(default)</code>: Default method.
</p>
</li>
<li> <p><code>perm_importance(ranger)</code>: Method for "ranger" models.
</p>
</li>
<li> <p><code>perm_importance(explainer)</code>: Method for DALEX "explainer".
</p>
</li>
</ul>
<h3>Losses</h3>

<p>The default <code>loss</code> is the "squared_error". Other choices:
</p>

<ul>
<li>
<p> "absolute_error": The absolute error is the loss corresponding to median regression.
</p>
</li>
<li>
<p> "poisson": Unit Poisson deviance, i.e., the loss function used in
Poisson regression. Actual values <code>y</code> and predictions must be non-negative.
</p>
</li>
<li>
<p> "gamma": Unit gamma deviance, i.e., the loss function of Gamma regression.
Actual values <code>y</code> and predictions must be positive.
</p>
</li>
<li>
<p> "logloss": The Log Loss is the loss function used in logistic regression,
and the top choice in probabilistic binary classification. Responses <code>y</code> and
predictions must be between 0 and 1. Predictions represent probabilities of
having a "1".
</p>
</li>
<li>
<p> "mlogloss": Multi-Log-Loss is the natural loss function in probabilistic multi-class
situations. If there are K classes and n observations, the predictions form
a (n x K) matrix of probabilities (with row-sums 1).
The observed values <code>y</code> are either passed as (n x K) dummy matrix,
or as discrete vector with corresponding levels.
The latter case is turned into a dummy matrix by a fast version of
<code>model.matrix(~ as.factor(y) + 0)</code>.
</p>
</li>
<li>
<p> A function with signature <code>f(actual, predicted)</code>, returning a numeric
vector or matrix of the same length as the input.
</p>
</li>
</ul>
<h3>References</h3>

<p>Fisher A., Rudin C., Dominici F. (2018). All Models are Wrong but many are Useful:
Variable Importance for Black-Box, Proprietary, or Misspecified Prediction
Models, using Model Class Reliance. Arxiv.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># MODEL 1: Linear regression
fit &lt;- lm(Sepal.Length ~ ., data = iris)
s &lt;- perm_importance(fit, X = iris, y = "Sepal.Length")

s
s$M
s$SE  # Standard errors are available thanks to repeated shuffling
plot(s)
plot(s, err_type = "SD")  # Standard deviations instead of standard errors

# Groups of features can be passed as named list
v &lt;- list(petal = c("Petal.Length", "Petal.Width"), species = "Species")
s &lt;- perm_importance(fit, X = iris, y = "Sepal.Length", v = v, verbose = FALSE)
s
plot(s)

# MODEL 2: Multi-response linear regression
fit &lt;- lm(as.matrix(iris[, 1:2]) ~ Petal.Length + Petal.Width + Species, data = iris)
s &lt;- perm_importance(fit, X = iris[, 3:5], y = iris[, 1:2], normalize = TRUE)
s
plot(s)
plot(s, swap_dim = TRUE, top_m = 2)
</code></pre>


</div>