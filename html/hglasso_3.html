<div class="container">

<table style="width: 100%;"><tr>
<td>hbn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Hub binary network</h2>

<h3>Description</h3>

<p>Estimates a binary network with hub nodes using a Lasso penalty and a sparse group Lasso penalty.  The estimated Theta matrix can be decomposed as Theta = Z + V + t(V), where Z is a sparse matrix and V is a matrix that contains hub nodes.  The details are given in Tan et al. (2014).
</p>


<h3>Usage</h3>

<pre><code class="language-R">hbn(X, lambda1, lambda2=100000, lambda3=100000, convergence = 1e-8
, maxiter = 1000, start = "cold", var.init = NULL, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>An n by p data matrix.  Cannot contain missing values.       
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda1</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix Z.  lambda=0 means no regularization.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix V.  lambda2=0 means no regularization.  The default value is lambda2=100000, encouraging V to be a zero matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda3</code></td>
<td>

<p>Non-negative regularization parameter for group lasso on the matrix V.  lambda3=0 means no regularization.  The default value is lambda3=100000, encouraging V to be a zero matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>convergence</code></td>
<td>

<p>Threshold for convergence.  Devault value is 1e-8. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>

<p>Maximum number of iterations of ADMM algorithm.  Default is 1000 iterations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>

<p>Type of start.  cold start is the default.  Using warm start, one can provide starting values for the parameters using object from hbn.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.init</code></td>
<td>

<p>Object from hbn that provides starting values for all the parameters when start="warm" is specified.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>

<p>Default value of trace=FALSE.  If trace=TRUE, every 10 iterations of the ADMM algorithm is printed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This implements hub binary network using ADMM algorithm (see Algorithm 1 and Section 5) in Tan et al. (2014).  The estimated Theta matrix can be decomposed into Z + V + t(V): Z is a sparse matrix and V is a matrix that contains dense columns, each column corresponding to a hub node.  
</p>
<p>The default value of lambda2=100000 and lambda3=100000 will yield the sparse binary network model estimate as in Hofling and Tibshirani  (2009) 'Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods'.   
</p>
<p>The tuning parameters lambda1 determines the sparsity of the matrix Z, lambda2 determines the sparsity of the selected hub nodes, and lambda3 determines the selection of hub nodes.  
</p>
<p>Within each iteration of the ADMM algorithm, we need to perform an iterative procedure to obtain an update for the matrix Theta since there is no closed form solution for Theta.  The Barzilai-Borwein method is used for this purpose (Barzilai and Borwein, 1988).  For details, see Algorithm 2 in Appendix F in Tan et al. (2014).    
</p>
<p>Note: we recommend using this function for moderate size network. For instance, network with 50-100 variables.  
</p>


<h3>Value</h3>

<p>an object of class hbn.  
</p>
<p>Among some internal variables, this object includes the elements 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Theta</code></td>
<td>
<p>Theta is the estimated inverse covariance matrix. Note that Theta = Z + V + t(V).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>V</code></td>
<td>
<p>V is the estimated matrix that contains hub nodes used to compute Theta.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z</code></td>
<td>
<p>Z is the estimated sparse matrix used to compute Theta.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hubind</code></td>
<td>
<p>Indices for features that are estimated to be hub nodes</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Kean Ming Tan and Karthik Mohan
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>
<p>Hofling, H. and Tibshirani, R. (2009). Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods. Journal of Machine Learning Research, 10:883-906. 
</p>
<p>Barzilai, J. and Borwein, J. (1988).  Two-point step size gradient methods. IMA Journal of Numerical Analysis, 8:141-148.
</p>


<h3>See Also</h3>

<p><code>image.hglasso</code>
<code>plot.hglasso</code>
<code>summary.hglasso</code>
<code>binaryMCMC</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">##############################################
# An implementation of Hub Binary Network
##############################################
#set.seed(1000)
#n=50
#p=5

# A network with 2 hubs
#network&lt;-HubNetwork(p,0.95,2,0.1,type="binary")
#Theta &lt;- network$Theta
#truehub &lt;- network$hubcol
# The four hub nodes have indices 4,5
#print(truehub)

# Generate data matrix x
#X &lt;- binaryMCMC(n,Theta,burnin=500,skip=100)

# Run Hub Binary Network to estimate Theta
#res1 &lt;- hbn(X,2,1,3,trace=TRUE)

# print out a summary of the object hbn
#summary(res1)

# We see that the estimated hub nodes have indices 1,5
# We successfully recover the hub nodes

# Plot the resulting network
# plot(res1) 
</code></pre>


</div>