<div class="container">

<table style="width: 100%;"><tr>
<td>plotROC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Plotting function illustrating the H-measure of classification performance.
</h2>

<h3>Description</h3>

<p>Plots ROC curves, weight distributions according to the H-measure and the AUC, and smoothed scoring distributions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">plotROC(results, which = 1, bw = "nrd0", cols = c("red",
                 "blue", "green", "magenta", "yellow", "forestgreen"),
                 greyscale = FALSE, lty = c(1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>results</code></td>
<td>

<p>An object of class "hmeasure", which will be the output of the "HMeasure" function applied on the scores of one or more classifiers on the same test dataset, and the true class labels of the test dataset. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>which</code></td>
<td>

<p>The type of plot to be produced. Option 1 yields the ROC curves and their convex hulls; option 2 shows the weights employed by the H-measure; option 3 displays the weights over costs that are implicitly employed by the AUC in this example for each classifier; and option 4 yields the smoothed density plots of the scoring distributions per class.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bw</code></td>
<td>

<p>The type of bandwidth to be used for the smoothed scoring distribution plot. See help(density) for more detail on possible settings of the "bw" parameter. We employ the same default setting as 'density()', namely "nrd0". 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cols</code></td>
<td>

<p>A list of colours to be used in order for the ROC and convex ROC hulls for each classifier. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>greyscale</code></td>
<td>

<p>A flag which if set to TRUE prints the plot on greyscale, with as much differentiation between classifiers as possible, to support black-and-white journal publications.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lty</code></td>
<td>

<p>The line type.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This plotting routine is meant to help users perform graphical comparisons of several classifiers, and to illustrate the difference between the H-measure and the AUC as aggregate measures of classification performance. Four types of plots are available:
</p>
<p>Option 1 plots the Receiver Operating Characteristic curves of each classifier in a continuous colored line, as well as their respective convex hulls in dotted lines. Additionally the ROC curve of the trivial, random classifier is reported, which is a diagonal line. 
</p>
<p>Option 2 plots the prior over misclassification costs employed by the H-measure. The mode of this prior, controlled by the severity ratio parameter, is indicated by a vertical line.
</p>
<p>Option 3 plots, for each classifier, the prior over misclassification costs implicitly employed by the AUC measure, when this latter is interpreted as an averaging operation over different choices of relative misclassification severity.
</p>
<p>Finally, option 4 plots, for each classifier, the (smoothed) empirical probability densities of the score conditional on either class label, i.e., p(s(x) | x = 0), where x is the feature vector and s(x) the score of that feature by a given classifier. Class 0 is plotted using a continuous line, and class 1 using a dotted line. 
</p>


<h3>Note</h3>

<p>This plotting function is provided to help illustrate the H-measure, and to serve as a stepping stone in a user's development of further custom plotting functions using the "data" attribute of hmeasure objects.
</p>


<h3>Author(s)</h3>

<p>Christoforos Anagnostopoulos &lt;canagnos@imperial.ac.uk&gt; and David J. Hand &lt;d.j.hand@imperial.ac.uk&gt;
</p>
<p>Maintainer: Christoforos Anagnostopoulos &lt;canagnos@imperial.ac.uk&gt;
</p>


<h3>References</h3>

<p>Hand, D.J. 2009. Measuring classifier performance: a coherent alternative to the area under the ROC curve. <em>Machine Learning</em>, <b>77</b>, 103–123.
</p>
<p>Hand, D.J. 2010. Evaluating diagnostic tests: the area under the ROC curve and the balance of errors. <em>Statistics in Medicine</em>, <b>29</b>, 1502–1510.
</p>
<p>Hand, D.J. and Anagnostopoulos, C. 2012. A better Beta for the H measure of classification performance. Preprint, arXiv:1202.2564v1
</p>


<h3>See Also</h3>

<p>summary.hmeasure, misclassCounts, relabel, HMeasure
</p>


<h3>Examples</h3>

<pre><code class="language-R">
  
# load the data
library(MASS) 
library(class) 
data(Pima.te) 

# split it into training and test
n &lt;- dim(Pima.te)[1] 
ntrain &lt;- floor(2*n/3) 
ntest &lt;- n-ntrain
pima.train &lt;- Pima.te[seq(1,n,3),]
pima.test &lt;- Pima.te[-seq(1,n,3),]
true.class&lt;-pima.test[,8]

# train an LDA classifier
pima.lda &lt;- lda(formula=type~., data=pima.train)
out.lda &lt;- predict(pima.lda,newdata=pima.test) 

# obtain the predicted labels and classification scores
scores.lda &lt;- out.lda$posterior[,2]

# train k-NN classifier
class.knn &lt;- knn(train=pima.train[,-8], test=pima.test[,-8],
  cl=pima.train$type, k=9, prob=TRUE, use.all=TRUE)
scores.knn &lt;- attr(class.knn,"prob")
# this is necessary because k-NN by default outputs
# the posterior probability of the winning class
scores.knn[class.knn=="No"] &lt;- 1-scores.knn[class.knn=="No"] 

# run the HMeasure function on the data frame of scores
scores &lt;- data.frame(LDA=scores.lda,kNN=scores.knn)
results &lt;- HMeasure(true.class,scores)


# produce the four different types of available plots
par(mfrow=c(2,2))
plotROC(results,which=1)
plotROC(results,which=2)
plotROC(results,which=3)
plotROC(results,which=4)


</code></pre>


</div>