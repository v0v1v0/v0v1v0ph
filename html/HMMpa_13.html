<div class="container">

<table style="width: 100%;"><tr>
<td>HMM_training</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Training of Hidden Markov Models
</h2>

<h3>Description</h3>

<p>Function to estimate the model specific parameters (<code>delta, gamma, distribution_theta</code>) for a hidden Markov model, given a time-series and a user-defined distribution class. Can also be used for model selection (selecting the optimal number of states <code>m</code>). See Details for more information. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">HMM_training (x, distribution_class, min_m = 2, max_m = 6, 
              n = 100, training_method = "EM", discr_logL = FALSE, 
              discr_logL_eps = 0.5, Mstep_numerical = FALSE, 
              dynamical_selection = TRUE, BW_max_iter = 50, 
              BW_limit_accuracy = 0.001, BW_print = TRUE,
              DNM_max_iter = 50, DNM_limit_accuracy = 0.001, 
              DNM_print = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>a vector object of length <code>T</code> containing observations of a time-series <code>x</code>, which are assumed to be realizations of the (hidden Markov state dependent) observation process of the HMM.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution_class</code></td>
<td>

<p>a single character string object with the abbreviated name of the $m$ observation distributions of the Markov dependent observation process.  The following distributions are supported:  Poisson (<code>pois</code>); generalized Poisson (<code>genpois</code>, only available for <code>training_method="numerical"</code>); normal (<code>norm</code>)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_m</code></td>
<td>
<p>minimum number of hidden states in the hidden Markov chain.  Default value is <code>2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_m</code></td>
<td>
<p>maximum number of hidden states in the hidden Markov chain.  Default value is <code>6</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>

<p>a single numerical value specifying the number of samples to find the best starting values for the training algorithm.  Default value is <code>n=100</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training_method</code></td>
<td>

<p>a logical value indicating whether the Baum-Welch algorithm (<code>"EM"</code>) or the method of direct numerical maximization (<code>"numerical"</code>) should be applied for estimating the model specific parameters. See <code>Baum_Welch_algorithm</code> and <code>direct_numerical_maximization</code> for further details.  
</p>
<p>Default is <code>training_method="EM"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>discr_logL</code></td>
<td>

<p>a logical object.  Default is <code>FALSE</code> for the general log-likelihood, <code>TRUE</code> for the discrete log-likelihood (for <code>distribution_class="norm"</code>).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>discr_logL_eps</code></td>
<td>

<p>a single numerical value, used to approximate the discrete log-likelihood for a hidden Markov model based on nomal distributions (for <code>"norm"</code>).  The default value is <code>0.5</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Mstep_numerical</code></td>
<td>

<p>a logical object indicating whether the Maximization Step of the Baum-Welch algorithm should be performed by numerical maximization. Default is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dynamical_selection</code></td>
<td>

<p>a logical value indicating whether the method of dynamical initial parameter selection should be applied (see Details).  Default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>BW_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations in the Baum-Welch algorithm.  Default value is <code>50</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>BW_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the Baum-Welch algorithm. Default value is is <code>0.001</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>BW_print</code></td>
<td>

<p>a logical object indicating whether the log-likelihood at each iteration-step shall be printed. Default is <code>TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DNM_max_iter</code></td>
<td>

<p>a single numerical value representing the maximum number of iterations of the numerical maximization using the nlm-function (used to perform the Maximization Step of the Baum-Welch-algorithm).  Default value is <code>50</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DNM_limit_accuracy</code></td>
<td>

<p>a single numerical value representing the convergence criterion of the numerical maximization algorithm using the nlm function (used to perform the Maximization Step of the Baum-Welch- algorithm).  Default value is <code>0.001</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DNM_print</code></td>
<td>

<p>a single numerical value to determine the level of printing of the <code>nlm</code>-function.  See <code>nlm</code>-function for further informations. The value <code>0</code> suppresses, that no printing will be outputted. Default value is <code>2</code> for full printing.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>More precisely, the function works as follows:
</p>
<p><b>Step 1:</b>
In a first step, the algorithm estimates the model specific parameters for different values of <code>m</code> (indeed for <code>min_m,...,max_m</code>) using either the function <code>Baum_Welch_algorithm</code> or 
</p>
<p><code>direct_numerical_maximization</code>. Therefore, the function first searches for plausible starting values by using the function <code>initial_parameter_training</code>. 
</p>
<p><b>Step 2:</b>
In a second step, this function evaluates the AIC and BIC values for each HMM (built in Step 1) using the functions <code>AIC_HMM</code> and <code>BIC_HMM</code>. Then, based on that values, this function decides for the most plausible number of states <code>m</code> (respectively for the most appropriate HMM for the given time-series of observations). In case when AIC and BIC claim for a different <code>m</code>, the algorithm decides for the smaller value for <code>m</code> (with the background to have a more simplistic model). 
If the user is intereseted in having a HMM with a fixed number for <code>m</code>, <code>min_m</code> and <code>max_m</code> have to be chosen equally (for instance <code>min_m=4=max_m</code> for a HMM with <code>m=4</code> hidden states).
</p>
<p>To speed up the parameter estimation for each <code class="reqn">m &gt; m_min</code>, the user can choose the method of dynamical initial parameter selection.
</p>
<p>If the method of dynamical intial parameter selection <b>is not applied</b>, the function 
</p>
<p><code>initial_parameter_training</code> will be called to find plausible starting values for each state 
<code class="reqn"> m \in \{min_m, \ldots, max_m\}</code>. <br></p>
<p>If the method of dynamical intial parameter selection <b>is applied</b>, then starting parameter values using the function <code>initial_parameter_training</code> will be found only for the first HMM (respectively the HMM with <code>m_min</code> states). The further starting parameter values for the next HMM (with <code>m+1</code> states and so on) are retained from the trained parameter values of the last HMM (with <code>m</code> states and so on). 
</p>


<h3>Value</h3>

<p><code>HMM_training</code> returns a list containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>trained_HMM_with_selected_m</code></td>
<td>

<p>a list object containg the key data of the optimal trained HMM (HMM with selected <code>m</code>) â€“ summarized output of the <code>Baum_Welch_algorithm</code> or 
</p>
<p><code>direct_numerical_maximization</code> algorithm, respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>list_of_all_initial_parameters</code></td>
<td>

<p>a list object containing the plausible starting values for all HMMs (one for each state <code>m</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>list_of_all_trained_HMMs</code></td>
<td>

<p>a list object containing all trained m-state-HMMs. See <code>Baum_Welch_algorithm</code> or <code>direct_numerical_maximization</code> for <code>training_method="EM"</code> or 
</p>
<p><code>training_method="numerical"</code>, respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>list_of_all_logLs_for_each_HMM_with_m_states</code></td>
<td>

<p>a list object containing all logarithmized Likelihoods of each trained HMM.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>list_of_all_AICs_for_each_HMM_with_m_states</code></td>
<td>

<p>a list object containing the AIC values of all trained HMMs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>list_of_all_BICs_for_each_HMM_with_m_states</code></td>
<td>

<p>a list object containing the BIC values of all trained HMMs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_selection_over_AIC</code></td>
<td>
<p>is logical.  <code>TRUE</code>, if model selection was based on AIC and <code>FALSE</code>, if model selection was based on BIC.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Vitali Witowski (2013).
</p>


<h3>References</h3>

<p>MacDonald, I. L.,  Zucchini, W. (2009) <em>Hidden Markov Models for Time Series: An Introduction Using R</em>, Boca Raton: Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code>initial_parameter_training</code>,
<code>Baum_Welch_algorithm</code>,
<code>direct_numerical_maximization</code>,
<code>AIC_HMM</code>,
<code>BIC_HMM</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
################################################################
### Fictitious observations ####################################
################################################################

x &lt;- c(1,16,19,34,22,6,3,5,6,3,4,1,4,3,5,7,9,8,11,11,
  14,16,13,11,11,10,12,19,23,25,24,23,20,21,22,22,18,7,
  5,3,4,3,2,3,4,5,4,2,1,3,4,5,4,5,3,5,6,4,3,6,4,8,9,12,
  9,14,17,15,25,23,25,35,29,36,34,36,29,41,42,39,40,43,
  37,36,20,20,21,22,23,26,27,28,25,28,24,21,25,21,20,21,
  11,18,19,20,21,13,19,18,20,7,18,8,15,17,16,13,10,4,9,
  7,8,10,9,11,9,11,10,12,12,5,13,4,6,6,13,8,9,10,13,13,
  11,10,5,3,3,4,9,6,8,3,5,3,2,2,1,3,5,11,2,3,5,6,9,8,5,
  2,5,3,4,6,4,8,15,12,16,20,18,23,18,19,24,23,24,21,26,
  36,38,37,39,45,42,41,37,38,38,35,37,35,31,32,30,20,39,
  40,33,32,35,34,36,34,32,33,27,28,25,22,17,18,16,10,9,
  5,12,7,8,8,9,19,21,24,20,23,19,17,18,17,22,11,12,3,9,
  10,4,5,13,3,5,6,3,5,4,2,5,1,2,4,4,3,2,1) 


## Train a poisson hidden Markov model using the Baum-Welch 
## algorithm for different number of states m=2,...,6

trained_HMMs &lt;- 
    HMM_training(x = x, 
      distribution_class = "pois", 
      min_m = 2, 
      max_m = 6, 
      training_method = "EM")


## Various output values for the HMM
names(trained_HMMs)

## Print details of the most plausible HMM for the given 
## time-series of observations
print(trained_HMMs$trained_HMM_with_selected_m)

## Print details of all trained HMMs (by this function) 
## for the given time-series of observations
print(trained_HMMs$list_of_all_trained_HMMs)

## Print the BIC-values of all trained HMMs for the given 
## time-series of observations  
print(trained_HMMs$list_of_all_BICs_for_each_HMM_with_m_states)

## Print the logL-values of all trained HMMs for the 
## given time-series of observations  
print(trained_HMMs$list_of_all_logLs_for_each_HMM_with_m_states)

</code></pre>


</div>