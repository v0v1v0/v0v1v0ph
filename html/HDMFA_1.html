<div class="container">

<table style="width: 100%;"><tr>
<td>alpha_PCA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Statistical Inference for High-Dimensional Matrix-Variate Factor Model
</h2>

<h3>Description</h3>

<p>This function is to fit the matrix factor model via the <code class="reqn">\alpha</code>-PCA method by conducting eigen-analysis of a weighted average of the sample mean and the column (row) sample covariance matrix through a hyper-parameter <code class="reqn">\alpha</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">alpha_PCA(X, m1, m2, alpha = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m1</code></td>
<td>

<p>A positive integer indicating the row factor numbers.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m2</code></td>
<td>

<p>A positive integer indicating the column factor numbers.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>

<p>A hyper-parameter balancing the information of the first and second moments    (<code class="reqn">\alpha \geq -1</code> ). The default is 0.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For the matrix factor models, Chen &amp; Fan (2021) propose an estimation procedure, i.e. <code class="reqn">\alpha</code>-PCA. The method aggregates the information in both first and second moments and extract it via a spectral method. In detail, for observations <code class="reqn">\bold{X}_t, t=1,2,\cdots,T</code>, define 
</p>
<p style="text-align: center;"><code class="reqn">\hat{\bold{M}}_R = \frac{1}{p_1 p_2} \left( (1+\alpha) \bar{\bold{X}} \bar{\bold{X}}^\top + \frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}}) (\bold{X}_t - \bar{\bold{X}})^\top \right),</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\bold{M}}_C = \frac{1}{p_1 p_2} \left( (1+\alpha) \bar{\bold{X}}^\top \bar{\bold{X}} + \frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}})^\top (\bold{X}_t - \bar{\bold{X}}) \right),</code>
</p>
 
<p>where <code class="reqn">\alpha \in</code> [-1,<code class="reqn">+\infty</code>], <code class="reqn">\bar{\bold{X}} = \frac{1}{T} \sum_{t=1}^T \bold{X}_t</code>, <code class="reqn">\frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}}) (\bold{X}_t - \bar{\bold{X}})^\top</code> and <code class="reqn">\frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}})^\top (\bold{X}_t - \bar{\bold{X}})</code> are the sample row and column covariance matrix, respectively. The loading matrices <code class="reqn">\bold{R}</code> and <code class="reqn">\bold{C}</code> are estimated as <code class="reqn">\sqrt{p_1}</code> times the top <code class="reqn">k_1</code> eigenvectors of <code class="reqn">\hat{\bold{M}}_R</code> and <code class="reqn">\sqrt{p_2}</code> times the top <code class="reqn">k_2</code> eigenvectors of <code class="reqn">\hat{\bold{M}}_C</code>, respectively. For details, see Chen &amp; Fan (2021). 
</p>


<h3>Value</h3>

<p>The return value is a list. In this list, it contains the following:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>F</code></td>
<td>
<p>The estimated factor matrix of dimension <code class="reqn">T \times m_1\times m_2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>
<p>The estimated row loading matrix of dimension <code class="reqn">p_1\times m_1</code>, satisfying <code class="reqn">\bold{R}^\top\bold{R}=p_1\bold{I}_{m_1}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C</code></td>
<td>
<p>The estimated column loading matrix of dimension <code class="reqn">p_2\times m_2</code>, satisfying <code class="reqn">\bold{C}^\top\bold{C}=p_2\bold{I}_{m_2}</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>Chen, E. Y., &amp; Fan, J. (2021). Statistical inference for high-dimensional matrix-variate factor models. Journal of the American Statistical Association, 1-18.
</p>


<h3>Examples</h3>

<pre><code class="language-R">   set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   #Estimate the factor matrices and loadings
   fit=alpha_PCA(X, k1, k2, alpha = 0)
   Rhat=fit$R 
   Chat=fit$C
   Fhat=fit$F
   
   #Estimate the common component
   CC=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC[t,,]=Rhat%*%Fhat[t,,]%*%t(Chat)
   }
   CC
</code></pre>


</div>