<div class="container">

<table style="width: 100%;"><tr>
<td>cv.hfr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross validation for a hierarchical feature regression</h2>

<h3>Description</h3>

<p>HFR is a regularized regression estimator that decomposes a least squares
regression along a supervised hierarchical graph, and shrinks the edges of the
estimated graph to regularize parameters. The algorithm leads to group shrinkage in the
regression parameters and a reduction in the effective model degrees of freedom.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cv.hfr(
  x,
  y,
  weights = NULL,
  kappa = seq(0, 1, by = 0.1),
  q = NULL,
  intercept = TRUE,
  standardize = TRUE,
  nfolds = 10,
  foldid = NULL,
  partial_method = c("pairwise", "shrinkage"),
  l2_penalty = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Input matrix or data.frame, of dimension <code class="reqn">(N\times p)</code>; each row is an observation vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector. If non-NULL, weighted least squares is used for the level-specific regressions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kappa</code></td>
<td>
<p>A vector of target effective degrees of freedom of the regression.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>Thinning parameter representing the quantile cut-off (in terms of contributed variance) above which to consider levels in the hierarchy. This can used to reduce the number of levels in high-dimensional problems. Default is no thinning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Should intercept be fitted. Default is <code>intercept=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Logical flag for <code>x</code> variable standardization prior to fitting the model. The coefficients are always returned on the original scale. Default is <code>standardize=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>The number of folds for k-fold cross validation. Default is <code>nfolds=10</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>foldid</code></td>
<td>
<p>An optional vector of values between <code>1</code> and <code>nfolds</code> identifying what fold each observation is in. If supplied, <code>nfolds</code> can be missing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partial_method</code></td>
<td>
<p>Indicate whether to use pairwise partial correlations, or shrinkage partial correlations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l2_penalty</code></td>
<td>
<p>Optional penalty for level-specific regressions (useful in high-dimensional case)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>hclust</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits an HFR to a grid of <code>kappa</code> hyperparameter values. The result is a
matrix of coefficients with one column for each hyperparameter. By evaluating all hyperparameters
in a single function, the speed of the cross-validation procedure is improved substantially (since
level-specific regressions are estimated only once).
</p>
<p>When <code>nfolds &gt; 1</code>, a cross validation is performed with shuffled data. Alternatively,
test slices can be passed to the function using the <code>foldid</code> argument. The result
of the cross validation is given by <code>best_kappa</code> in the output object.
</p>


<h3>Value</h3>

<p>A 'cv.hfr' regression object.
</p>


<h3>Author(s)</h3>

<p>Johann Pfitzinger
</p>


<h3>References</h3>

<p>Pfitzinger, Johann (2024). Cluster Regularization via a Hierarchical Feature Regression. _Econometrics and Statistics_ (in press). URL https://doi.org/10.1016/j.ecosta.2024.01.003.
</p>


<h3>See Also</h3>

<p><code>hfr</code>, <code>coef</code>, <code>plot</code> and <code>predict</code> methods
</p>


<h3>Examples</h3>

<pre><code class="language-R">x = matrix(rnorm(100 * 20), 100, 20)
y = rnorm(100)
fit = cv.hfr(x, y, kappa = seq(0, 1, by = 0.1))
coef(fit)

</code></pre>


</div>